{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayaramch743/AI-projects/blob/main/mf_fin_ui_deep_gradio\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qnHvQndcVuTp",
        "outputId": "72ae0ffc-2556-4e16-a983-c5ef39b755b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\r0% [Working]\u001b[0m\r            \rHit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            ">>> NVIDIA GPU installed.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Fetched 257 kB in 2s (153 kB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "31 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "pciutils is already the newest version (1:3.7.0-6).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n"
          ]
        }
      ],
      "source": [
        "!sudo apt update\n",
        "!sudo apt install -y pciutils\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "KLt5saKUWLVR"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "import subprocess\n",
        "import time\n",
        "\n",
        "def run_ollama_serve():\n",
        "  subprocess.Popen([\"ollama\", \"serve\"])\n",
        "\n",
        "thread = threading.Thread(target=run_ollama_serve)\n",
        "thread.start()\n",
        "time.sleep(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJib5YLdWcek",
        "outputId": "0f40c01f-11c3-4b9b-b459-7ffcf314342e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠧ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
            "pulling 6e9f90f02bb3... 100% ▕▏ 9.0 GB                         \u001b[K\n",
            "pulling 369ca498f347... 100% ▕▏  387 B                         \u001b[K\n",
            "pulling 6e4c38e1172f... 100% ▕▏ 1.1 KB                         \u001b[K\n",
            "pulling f4d24e9138dd... 100% ▕▏  148 B                         \u001b[K\n",
            "pulling 3c24b0c80794... 100% ▕▏  488 B                         \u001b[K\n",
            "verifying sha256 digest \u001b[K\n",
            "writing manifest \u001b[K\n",
            "success \u001b[K\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ],
      "source": [
        "!ollama pull deepseek-r1:14b\n",
        "#!ollama run deepseek-v3:671b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hojk52IW411",
        "outputId": "fa7d4157-3396-49db-d138-552029cb2bba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-deepseek in /usr/local/lib/python3.11/dist-packages (0.1.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.47 in /usr/local/lib/python3.11/dist-packages (from langchain-deepseek) (0.3.51)\n",
            "Requirement already satisfied: langchain-openai<1.0.0,>=0.3.9 in /usr/local/lib/python3.11/dist-packages (from langchain-deepseek) (0.3.12)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.3.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.11.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.70.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (0.9.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.23.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.4.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (2024.11.6)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai<1.0.0,>=0.3.9->langchain-deepseek) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.47->langchain-deepseek) (2.3.0)\n",
            "Requirement already satisfied: langchain-ollama in /usr/local/lib/python3.11/dist-packages (0.3.1)\n",
            "Requirement already satisfied: ollama<1,>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.4.7)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain-ollama) (0.3.51)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.3.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.11.2)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama<1,>=0.4.4->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.10.16)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.51->langchain-ollama) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama<1,>=0.4.4->langchain-ollama) (1.3.1)\n",
            "Requirement already satisfied: langgraph in /usr/local/lib/python3.11/dist-packages (0.3.29)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.3.51)\n",
            "Requirement already satisfied: langgraph-checkpoint<3.0.0,>=2.0.10 in /usr/local/lib/python3.11/dist-packages (from langgraph) (2.0.24)\n",
            "Requirement already satisfied: langgraph-prebuilt<0.2,>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.8)\n",
            "Requirement already satisfied: langgraph-sdk<0.2.0,>=0.1.42 in /usr/local/lib/python3.11/dist-packages (from langgraph) (0.1.61)\n",
            "Requirement already satisfied: xxhash<4.0.0,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (0.3.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.1->langgraph) (2.11.2)\n",
            "Requirement already satisfied: ormsgpack<2.0.0,>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from langgraph-checkpoint<3.0.0,>=2.0.10->langgraph) (1.9.1)\n",
            "Requirement already satisfied: httpx>=0.25.2 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10.16)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.1->langgraph) (3.0.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.32.3)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.1->langgraph) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4,>=0.1->langgraph) (2.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.2.0,>=0.1.42->langgraph) (1.3.1)\n",
            "Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.51)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.70.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.3.24)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.11.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Requirement already satisfied: langchain-google-community in /usr/local/lib/python3.11/dist-packages (2.0.7)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.24.1 in /usr/local/lib/python3.11/dist-packages (from langchain-google-community) (2.24.2)\n",
            "Requirement already satisfied: google-api-python-client<3.0.0,>=2.161.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-community) (2.164.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from langchain-google-community) (2.4.3)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-community) (1.71.0)\n",
            "Requirement already satisfied: langchain-community<0.4.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-community) (0.3.21)\n",
            "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from langchain-google-community) (0.3.51)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.24.1->langchain-google-community) (1.69.2)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.24.1->langchain-google-community) (5.29.4)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.24.1->langchain-google-community) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.24.1->langchain-google-community) (2.38.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=2.24.1->langchain-google-community) (2.32.3)\n",
            "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client<3.0.0,>=2.161.0->langchain-google-community) (0.22.0)\n",
            "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client<3.0.0,>=2.161.0->langchain-google-community) (0.2.0)\n",
            "Requirement already satisfied: uritemplate<5,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from google-api-python-client<3.0.0,>=2.161.0->langchain-google-community) (4.1.1)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.23 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.3.23)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.3.24)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community<0.4.0,>=0.3.0->langchain-google-community) (2.0.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-community) (4.13.1)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-google-community) (2.11.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.9.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (4.9)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<1.dev0,>=0.19.0->google-api-python-client<3.0.0,>=2.161.0->langchain-google-community) (3.2.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-google-community) (3.0.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.23->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.3.8)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-google-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-google-community) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-google-community) (0.4.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (0.14.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=2.24.1->langchain-google-community) (0.6.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community<0.4.0,>=0.3.0->langchain-google-community) (1.3.1)\n",
            "Requirement already satisfied: pymupdf4llm in /usr/local/lib/python3.11/dist-packages (0.0.21)\n",
            "Requirement already satisfied: pymupdf>=1.25.5 in /usr/local/lib/python3.11/dist-packages (from pymupdf4llm) (1.25.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip -qq install langchain langchain-core langchain-community\n",
        "!pip install ollama beautifulsoup4 chromadb gradio -q\n",
        "!pip install pypdf\n",
        "!pip install -U langchain-deepseek\n",
        "!pip install langchain-ollama\n",
        "!pip install langgraph\n",
        "!pip install langchain-openai\n",
        "!pip install langchain-google-community\n",
        "!pip install pymupdf4llm\n",
        "!pip install matplotlib"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Python library\n",
        "!pip install graphviz\n",
        "\n",
        "# Install Graphviz system dependency\n",
        "# On Debian/Ubuntu:\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install graphviz\n",
        "# On macOS (using Homebrew):\n",
        "#!brew install graphviz\n",
        "# On Windows:\n",
        "# Download from: https://graphviz.org/download/\n",
        "# Add the Graphviz 'bin' directory to your system's PATH environment variable."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMTWY3vLFM89",
        "outputId": "f8a06acb-65ba-4580-e101-aa025f44c6dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (0.20.3)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "graphviz is already the newest version (2.42.2-6ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "import os # Already imported in your script\n",
        "\n",
        "# Make sure Graphviz binaries are in the system PATH or specify the path explicitly\n",
        "# Example if installed somewhere non-standard (adjust path as needed):\n",
        "# os.environ[\"PATH\"] += os.pathsep + 'C:/Program Files/Graphviz/bin'\n",
        "\n",
        "def generate_architecture_diagram(filename=\"langchain_architecture\"):\n",
        "    \"\"\"\n",
        "    Generates a diagram visualizing the Langchain component interactions.\n",
        "    \"\"\"\n",
        "    dot = graphviz.Digraph(comment='Langchain Financial Audit Assistant Architecture', format='png')\n",
        "    dot.attr(rankdir='TB', label='Langchain Financial Audit Assistant Architecture', fontsize='20')\n",
        "\n",
        "    # Define styles\n",
        "    app_style = {'shape': 'component', 'style': 'filled', 'fillcolor': '#a7d5ed'} # Light blue\n",
        "    agent_style = {'shape': 'oval', 'style': 'filled', 'fillcolor': '#f0e442'} # Yellow\n",
        "    tool_style = {'shape': 'box', 'style': 'filled', 'fillcolor': '#8fbc8f'} # Dark Sea Green\n",
        "    llm_style = {'shape': 'cds', 'style': 'filled', 'fillcolor': '#ffcc99'} # Light Orange\n",
        "    data_style = {'shape': 'folder', 'style': 'filled', 'fillcolor': '#cccccc'} # Grey\n",
        "    process_style = {'shape': 'ellipse', 'style': 'dashed'} # Dashed for internal processes\n",
        "\n",
        "    # Main Application / Framework\n",
        "    with dot.subgraph(name='cluster_app') as app_cluster:\n",
        "        app_cluster.attr(label='Application Framework', style='filled', color='lightgrey')\n",
        "        app_cluster.node('gradio_ui', 'Gradio UI', **app_style)\n",
        "        app_cluster.node('process_input', 'process_input()', **process_style)\n",
        "        app_cluster.node('custom_executor', 'custom_agent_executor()', **agent_style) # Treat as main agent logic hub\n",
        "        app_cluster.edge('gradio_ui', 'process_input', label='User Input (Questions, Files)')\n",
        "        app_cluster.edge('process_input', 'custom_executor', label='Calls')\n",
        "        app_cluster.edge('custom_executor', 'process_input', label='Returns Response')\n",
        "        app_cluster.edge('process_input', 'gradio_ui', label='Displays Output')\n",
        "\n",
        "    # Core LLM and Embeddings\n",
        "    with dot.subgraph(name='cluster_llm') as llm_cluster:\n",
        "        llm_cluster.attr(label='Core Models (Ollama)', style='filled', color='#fff0f0')\n",
        "        llm_cluster.node('llm', 'ChatOllama (deepseek-r1:14b)', **llm_style)\n",
        "        llm_cluster.node('embeddings', 'OllamaEmbeddings (deepseek-r1:14b)', **llm_style)\n",
        "\n",
        "    # Data Handling\n",
        "    with dot.subgraph(name='cluster_data') as data_cluster:\n",
        "        data_cluster.attr(label='Data Sources & Processing', style='filled', color='#f0f8ff') # Alice Blue\n",
        "        data_cluster.node('pdf_files', 'PDF Files', **data_style)\n",
        "        data_cluster.node('pymupdf', 'PyMuPDF Extraction', **process_style)\n",
        "        data_cluster.node('text_splitter', 'RecursiveCharacterTextSplitter', **process_style)\n",
        "        data_cluster.node('vectorstore', 'Chroma Vector Store', **data_style)\n",
        "\n",
        "        data_cluster.edge('pdf_files', 'pymupdf', label='Input')\n",
        "        data_cluster.edge('pymupdf', 'text_splitter', label='Extracted Text')\n",
        "        data_cluster.edge('text_splitter', 'vectorstore', label='Creates Chunks')\n",
        "        data_cluster.edge('vectorstore', 'embeddings', label='Uses for Indexing/Query', style='dashed')\n",
        "        # Also link process_input to vector store creation\n",
        "        dot.edge('process_input', 'vectorstore', label='Creates/Updates DB', style='dashed')\n",
        "\n",
        "\n",
        "    # Tools\n",
        "    with dot.subgraph(name='cluster_tools') as tool_cluster:\n",
        "        tool_cluster.attr(label='Tools', style='filled', color='#e0ffe0') # Honeydew\n",
        "        tool_cluster.node('rag_tool', 'Knowledge Base (RAG)', **tool_style)\n",
        "        tool_cluster.node('financial_calc_tool', 'Financial Calculator/Status', **tool_style)\n",
        "        tool_cluster.node('web_search_tool', 'Web Search (Google)', **tool_style)\n",
        "        tool_cluster.node('anomaly_tool', 'Anomaly Detector', **tool_style)\n",
        "        tool_cluster.node('pdf_extract_tool', 'PDF Extractor (Tool Wrapper)', **tool_style)\n",
        "        tool_cluster.node('financial_analyzer_class', 'FinancialAnalyzer Class', **process_style) # Internal logic used by tools\n",
        "\n",
        "    # Connections: Agent Logic -> Tools -> Resources/LLM\n",
        "    dot.edge('custom_executor', 'financial_calc_tool', label='Uses')\n",
        "    dot.edge('custom_executor', 'rag_tool', label='Uses')\n",
        "    dot.edge('custom_executor', 'web_search_tool', label='Uses (Fallback)')\n",
        "    dot.edge('custom_executor', 'anomaly_tool', label='Uses (Validation)')\n",
        "    dot.edge('custom_executor', 'pdf_extract_tool', label='Uses (Direct Extraction)')\n",
        "\n",
        "    dot.edge('rag_tool', 'vectorstore', label='Retrieves Docs')\n",
        "    dot.edge('rag_tool', 'llm', label='Synthesizes Answer')\n",
        "\n",
        "    dot.edge('financial_calc_tool', 'financial_analyzer_class', label='Uses Logic', style='dashed')\n",
        "    dot.edge('anomaly_tool', 'llm', label='May use LLM (Implicitly via Response)', style='dashed') # Less direct, depends on impl.\n",
        "\n",
        "    dot.edge('pdf_extract_tool', 'pymupdf', label='Calls Extraction', style='dashed')\n",
        "\n",
        "    # Direct calls from executor (not standard agent loop, but reflects your custom code)\n",
        "    dot.edge('custom_executor', 'llm', label='May Call Directly (Fallback/Refinement)', style='dotted')\n",
        "\n",
        "    # Render the graph\n",
        "    try:\n",
        "        dot.render(filename, view=False) # Set view=True to automatically open the image\n",
        "        print(f\"Architecture diagram saved as {filename}.png\")\n",
        "    except graphviz.backend.ExecutableNotFound:\n",
        "        print(\"Graphviz executable not found. Make sure Graphviz is installed and in your system's PATH.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during diagram generation: {e}\")\n",
        "\n",
        "# --- How to use it: ---\n",
        "# 1. Make sure Graphviz is installed (see step 1).\n",
        "# 2. Call this function somewhere in your script, e.g., at the end or near the main block.\n",
        "#    It's safe to call even within the Gradio script, as it just generates a file.\n",
        "# Example call (place this appropriately):\n",
        "# if __name__ == \"__main__\":\n",
        "#     generate_architecture_diagram() # Generate the diagram before launching Gradio\n",
        "#     demo.launch(debug=True)\n",
        "\n",
        "# Or just run this function standalone after installing graphviz\n",
        "# generate_architecture_diagram()"
      ],
      "metadata": {
        "id": "k-JK24TJFe1S"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import fitz  # PyMuPDF\n",
        "from typing import List, Dict, Any, Optional, Tuple\n",
        "import re\n",
        "\n",
        "import gradio as gr\n",
        "from langchain.agents import AgentExecutor, create_react_agent\n",
        "from langchain.tools import Tool\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader, PyPDFDirectoryLoader\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_ollama import ChatOllama\n",
        "from langchain_ollama import OllamaEmbeddings\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_core.tools import tool\n",
        "from langchain_google_community import GoogleSearchAPIWrapper\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import tempfile\n",
        "from langchain_core.output_parsers import BaseOutputParser\n",
        "from google.colab import userdata\n",
        "\n",
        "google_api_key = userdata.get('GOOGLE_API_KEY')\n",
        "google_cse_id = userdata.get('GOOGLE_CSE_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = google_api_key\n",
        "os.environ[\"GOOGLE_CSE_ID\"] = google_cse_id\n",
        "\n",
        "# Initialize components\n",
        "llm = ChatOllama(model=\"deepseek-r1:14b\")\n",
        "embeddings = OllamaEmbeddings(model=\"deepseek-r1:14b\")\n",
        "\n",
        "class ThinkTagOutputParser(BaseOutputParser):\n",
        "    def parse(self, text: str):\n",
        "        # First try to extract final answer if present\n",
        "        final_answer_match = re.search(r'Final Answer:\\s*(.*?)(?:\\n|$)', text, re.DOTALL | re.IGNORECASE)\n",
        "        if final_answer_match:\n",
        "            return final_answer_match.group(1).strip()\n",
        "\n",
        "        # Try to extract observation if present\n",
        "        observation_match = re.search(r'Observation:\\s*(.*?)(?:\\n|$)', text, re.DOTALL | re.IGNORECASE)\n",
        "        if observation_match:\n",
        "            return observation_match.group(1).strip()\n",
        "\n",
        "        # Remove any think tags and their content\n",
        "        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)\n",
        "\n",
        "        # Remove any remaining action/thought formatting if present\n",
        "        text = re.sub(r'Action:\\s*.*?\\nAction Input:\\s*\".*?\"', '', text, flags=re.DOTALL)\n",
        "        text = re.sub(r'Thought:\\s*.*?\\n', '', text, flags=re.DOTALL)\n",
        "\n",
        "        return text.strip()\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        return \"think_tag_parser\"\n",
        "\n",
        "class FinancialAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.financial_terms = {\n",
        "            'gross_potential_rent': ['gross potential rent', 'gpr'],\n",
        "            'effective_gross_income': ['effective gross income', 'egi'],\n",
        "            'net_operating_income': ['net operating income', 'noi'],\n",
        "            'debt_service_coverage_ratio': ['debt service coverage ratio', 'dscr'],\n",
        "            'loan_to_value': ['loan to value', 'ltv'],\n",
        "            'capitalization_rate': ['capitalization rate', 'cap rate'],\n",
        "            'ebitda': ['ebitda'],\n",
        "            'return_on_investment': ['return on investment', 'roi'],\n",
        "            'operating_expenses': ['operating expenses', 'opex'],\n",
        "            'vacancy_rate': ['vacancy rate']\n",
        "        }\n",
        "\n",
        "    def calculate_metrics(self, metrics: dict, question: str) -> dict:\n",
        "        \"\"\"Calculate financial metrics based on available data.\"\"\"\n",
        "        results = {}\n",
        "        question_lower = question.lower()\n",
        "\n",
        "        # Extract years from available data\n",
        "        years = []\n",
        "        for metric_data in metrics.values():\n",
        "            if isinstance(metric_data, dict):\n",
        "                years.extend(metric_data.keys())\n",
        "        years = sorted(list(set(years))) if years else ['Year 1', 'Year 2', 'Year 3', 'Year 4', 'Year 5']\n",
        "\n",
        "        # DSCR calculation\n",
        "        if any(term in question_lower for term in self.financial_terms['debt_service_coverage_ratio']):\n",
        "            if 'net_operating_income' in metrics and 'debt_service' in metrics:\n",
        "                for year in years:\n",
        "                    if isinstance(metrics['net_operating_income'], dict) and isinstance(metrics['debt_service'], dict):\n",
        "                        if year in metrics['net_operating_income'] and year in metrics['debt_service']:\n",
        "                            results[f'DSCR {year}'] = metrics['net_operating_income'][year] / metrics['debt_service'][year]\n",
        "                    elif not isinstance(metrics['net_operating_income'], dict) and not isinstance(metrics['debt_service'], dict):\n",
        "                        results['DSCR'] = metrics['net_operating_income'] / metrics['debt_service']\n",
        "\n",
        "        # Cap Rate calculation\n",
        "        if any(term in question_lower for term in self.financial_terms['capitalization_rate']):\n",
        "            if 'net_operating_income' in metrics and 'property_value' in metrics:\n",
        "                if isinstance(metrics['net_operating_income'], dict):\n",
        "                    for year in years:\n",
        "                        if year in metrics['net_operating_income']:\n",
        "                            results[f'Cap Rate {year}'] = (metrics['net_operating_income'][year] / metrics['property_value']) * 100\n",
        "                else:\n",
        "                    results['Cap Rate'] = (metrics['net_operating_income'] / metrics['property_value']) * 100\n",
        "\n",
        "        # LTV calculation\n",
        "        if any(term in question_lower for term in self.financial_terms['loan_to_value']):\n",
        "            if 'loan_amount' in metrics and 'property_value' in metrics:\n",
        "                results['LTV'] = (metrics['loan_amount'] / metrics['property_value']) * 100\n",
        "\n",
        "        return results\n",
        "\n",
        "financial_analyzer = FinancialAnalyzer()\n",
        "\n",
        "def extract_with_pymupdf(file_path: str) -> Tuple[str, List[Dict]]:\n",
        "    \"\"\"Enhanced PDF extraction with better table handling.\"\"\"\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        full_text = \"\"\n",
        "        extracted_tables = []\n",
        "\n",
        "        for page in doc:\n",
        "            # Extract text with formatting\n",
        "            blocks = page.get_text(\"dict\")[\"blocks\"]\n",
        "            for b in blocks:\n",
        "                if b[\"type\"] == 0:  # Text block\n",
        "                    for line in b[\"lines\"]:\n",
        "                        line_text = \"\"\n",
        "                        for span in line[\"spans\"]:\n",
        "                            text = span[\"text\"]\n",
        "                            if span[\"flags\"] & 2**4:  # Bold\n",
        "                                text = f\"**{text}**\"\n",
        "                            if span[\"flags\"] & 2**1:  # Italic\n",
        "                                text = f\"*{text}*\"\n",
        "                            line_text += text + \" \"\n",
        "                        full_text += line_text.strip() + \"\\n\"\n",
        "\n",
        "            # Enhanced table extraction\n",
        "            tables = page.find_tables()\n",
        "            if tables.tables:\n",
        "                for table in tables.tables:\n",
        "                    table_data = table.extract()\n",
        "                    if table_data:\n",
        "                        # Convert table to markdown format\n",
        "                        table_md = \"\\n\".join([\"\\t\".join(row) for row in table_data])\n",
        "                        full_text += f\"\\n\\nTABLE:\\n{table_md}\\n\"\n",
        "                        extracted_tables.append({\n",
        "                            \"headers\": table_data[0] if len(table_data) > 0 else [],\n",
        "                            \"rows\": table_data[1:] if len(table_data) > 1 else [],\n",
        "                            \"bbox\": table.bbox\n",
        "                        })\n",
        "\n",
        "        return full_text, extracted_tables\n",
        "    except Exception as e:\n",
        "        return f\"Error processing PDF: {str(e)}\", []\n",
        "\n",
        "def extract_numeric_value(text: str) -> Optional[float]:\n",
        "    \"\"\"Robust numeric value extraction from text.\"\"\"\n",
        "    if not text:\n",
        "        return None\n",
        "    try:\n",
        "        # Handle various number formats ($1,000.00, 25%, etc.)\n",
        "        clean_text = text.replace('$', '').replace(',', '').replace('%', '').strip()\n",
        "        return float(clean_text)\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "def extract_financial_metrics(file_path: str) -> dict:\n",
        "    \"\"\"Enhanced financial metrics extraction with precise handling.\"\"\"\n",
        "    full_text, tables = extract_with_pymupdf(file_path)\n",
        "    if isinstance(full_text, str) and full_text.startswith(\"Error\"):\n",
        "        try:\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            docs = loader.load()\n",
        "            full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            tables = []\n",
        "        except Exception as e:\n",
        "            return {'error': f\"Failed to extract text: {str(e)}\"}\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    # First process tables for structured data\n",
        "    for table in tables:\n",
        "        headers = [h.lower().strip() for h in table[\"headers\"]]\n",
        "        rows = table[\"rows\"]\n",
        "\n",
        "        # Identify columns with financial data\n",
        "        value_columns = []\n",
        "        year_column = None\n",
        "\n",
        "        for i, header in enumerate(headers):\n",
        "            if any(term in header for term in ['year', 'period', 'date']):\n",
        "                year_column = i\n",
        "            elif any(term in header for term in ['amount', 'value', 'income', 'expense', 'rate', 'rent']):\n",
        "                value_columns.append(i)\n",
        "\n",
        "        # Extract data from each row\n",
        "        for row in rows:\n",
        "            year = None\n",
        "            if year_column is not None and year_column < len(row):\n",
        "                year = row[year_column].strip()\n",
        "\n",
        "            for col in value_columns:\n",
        "                if col < len(row):\n",
        "                    metric_name = headers[col].replace(' ', '_')\n",
        "                    value = extract_numeric_value(row[col])\n",
        "                    if value is not None:\n",
        "                        if year:\n",
        "                            if metric_name not in metrics:\n",
        "                                metrics[metric_name] = {}\n",
        "                            metrics[metric_name][year] = value\n",
        "                        else:\n",
        "                            metrics[metric_name] = value\n",
        "\n",
        "    # Then extract from text for standalone metrics with improved patterns\n",
        "    financial_patterns = [\n",
        "        (r'(gross\\s+potential\\s+rent|gpr)[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'gross_potential_rent'),\n",
        "        (r'(net\\s+operating\\s+income|noi)[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'net_operating_income'),\n",
        "        (r'(property|asset)[\\s-]*value[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'property_value'),\n",
        "        (r'(loan|mortgage)[\\s-]*amount[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'loan_amount'),\n",
        "        (r'(total\\s+operating\\s+expenses|opex)[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'operating_expenses'),\n",
        "        (r'(vacancy\\s+rate)[:\\s]*([\\d,\\.]+)\\s*%', 'vacancy_rate'),\n",
        "        (r'(effective\\s+gross\\s+income|egi)[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'effective_gross_income'),\n",
        "        (r'(debt\\s+service)[:\\s]*[\\$]?\\s*([\\d,\\.]+)', 'debt_service'),\n",
        "        (r'(interest\\s+rate)[:\\s]*([\\d,\\.]+)\\s*%', 'interest_rate')\n",
        "    ]\n",
        "\n",
        "    for pattern, metric in financial_patterns:\n",
        "        matches = re.finditer(pattern, full_text, re.IGNORECASE)\n",
        "        for match in matches:\n",
        "            value = extract_numeric_value(match.group(2))\n",
        "            if value is not None and metric not in metrics:\n",
        "                # Check for year context in surrounding text\n",
        "                context = full_text[max(0, match.start()-100):match.end()+100]\n",
        "                year_match = re.search(r'(20\\d{2})|(year|yr|period)\\s*(\\d+)', context, re.IGNORECASE)\n",
        "                if year_match:\n",
        "                    year = year_match.group(1) or f\"Year {year_match.group(3)}\"\n",
        "                    if metric not in metrics:\n",
        "                        metrics[metric] = {}\n",
        "                    metrics[metric][year] = value\n",
        "                else:\n",
        "                    metrics[metric] = value\n",
        "\n",
        "    # If no metrics found, try a more aggressive text search\n",
        "    if not metrics:\n",
        "        # Look for common financial statement patterns\n",
        "        statement_patterns = [\n",
        "            (r'(gross\\s+potential\\s+rent|gpr)\\s*[\\$]?([\\d,\\.]+)', 'gross_potential_rent'),\n",
        "            (r'(net\\s+operating\\s+income|noi)\\s*[\\$]?([\\d,\\.]+)', 'net_operating_income'),\n",
        "            (r'(property\\s+value)\\s*[\\$]?([\\d,\\.]+)', 'property_value')\n",
        "        ]\n",
        "\n",
        "        for pattern, metric in statement_patterns:\n",
        "            matches = re.finditer(pattern, full_text, re.IGNORECASE)\n",
        "            for match in matches:\n",
        "                value = extract_numeric_value(match.group(2))\n",
        "                if value is not None and metric not in metrics:\n",
        "                    metrics[metric] = value\n",
        "\n",
        "    return metrics if metrics else {'error': 'No financial metrics found in document'}\n",
        "\n",
        "def answer_financial_question(question: str, file_paths: List[str]) -> str:\n",
        "    \"\"\"Precise financial question answering with direct metric extraction.\"\"\"\n",
        "    if not file_paths:\n",
        "        return \"No documents available to extract financial data.\"\n",
        "\n",
        "    try:\n",
        "        metrics = extract_financial_metrics(file_paths[0])\n",
        "        if 'error' in metrics:\n",
        "            # If direct extraction failed, try RAG approach as fallback\n",
        "            rag_response = answer_with_rag(question)\n",
        "            if rag_response not in [\"KNOWLEDGE_BASE_NOT_AVAILABLE\", \"NO_RELEVANT_DOCUMENTS\", \"ANSWER_NOT_IN_DOCUMENTS\"]:\n",
        "                return rag_response\n",
        "            return f\"Could not extract financial data from the document. {metrics['error']}\"\n",
        "\n",
        "        question_lower = question.lower()\n",
        "        response = \"\"\n",
        "\n",
        "        # Check for specific financial metrics in question\n",
        "        for metric, terms in financial_analyzer.financial_terms.items():\n",
        "            if any(term in question_lower for term in terms):\n",
        "                if metric in metrics:\n",
        "                    display_name = ' '.join([word.capitalize() for word in metric.split('_')])\n",
        "                    if isinstance(metrics[metric], dict):\n",
        "                        response = f\"{display_name} by period:\\n\"\n",
        "                        for period, value in metrics[metric].items():\n",
        "                            if 'rate' in metric:\n",
        "                                response += f\"{period}: {value:.2f}%\\n\"\n",
        "                            else:\n",
        "                                response += f\"{period}: ${value:,.2f}\\n\"\n",
        "                    else:\n",
        "                        if 'rate' in metric:\n",
        "                            response = f\"{display_name}: {metrics[metric]:.2f}%\"\n",
        "                        else:\n",
        "                            response = f\"{display_name}: ${metrics[metric]:,.2f}\"\n",
        "                    break\n",
        "\n",
        "        if not response:\n",
        "            # If no specific metric found, provide available metrics\n",
        "            response = \"Available financial metrics:\\n\"\n",
        "            for metric, values in metrics.items():\n",
        "                if metric == 'error':\n",
        "                    continue\n",
        "                display_name = ' '.join([word.capitalize() for word in metric.split('_')])\n",
        "                if isinstance(values, dict):\n",
        "                    response += f\"\\n{display_name} by period:\\n\"\n",
        "                    for period, val in values.items():\n",
        "                        if 'rate' in metric:\n",
        "                            response += f\"{period}: {val:.2f}%\\n\"\n",
        "                        else:\n",
        "                            response += f\"{period}: ${val:,.2f}\\n\"\n",
        "                else:\n",
        "                    if 'rate' in metric:\n",
        "                        response += f\"{display_name}: {values:.2f}%\\n\"\n",
        "                    else:\n",
        "                        response += f\"{display_name}: ${values:,.2f}\\n\"\n",
        "\n",
        "        # Add calculations if relevant\n",
        "        calculations = financial_analyzer.calculate_metrics(metrics, question)\n",
        "        if calculations:\n",
        "            response += \"\\nCalculated Metrics:\\n\"\n",
        "            for metric, value in calculations.items():\n",
        "                if '%' in metric:\n",
        "                    response += f\"{metric}: {value:.2f}%\\n\"\n",
        "                else:\n",
        "                    response += f\"{metric}: {value:.2f}\\n\"\n",
        "\n",
        "        return response if response else \"Could not find relevant financial data for this question.\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"Error processing financial data: {str(e)}\"\n",
        "\n",
        "\n",
        "# Global variable for vector store\n",
        "vectorstore = None\n",
        "\n",
        "def create_vector_db_from_files(files: List[str]) -> Chroma:\n",
        "    \"\"\"Create a vector database from uploaded PDF files.\"\"\"\n",
        "    docs = []\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "\n",
        "    for file_path in files:\n",
        "        extracted_text, _ = extract_with_pymupdf(file_path)\n",
        "        if isinstance(extracted_text, str) and extracted_text.startswith(\"Error\"):\n",
        "            print(f\"PyMuPDF extraction failed, falling back to PyPDFLoader: {extracted_text}\")\n",
        "            loader = PyPDFLoader(file_path)\n",
        "            docs.extend(loader.load())\n",
        "        else:\n",
        "            from langchain_core.documents import Document\n",
        "            docs.append(Document(page_content=extracted_text, metadata={\"source\": file_path}))\n",
        "\n",
        "    splits = text_splitter.split_documents(docs)\n",
        "    persist_directory = tempfile.mkdtemp()\n",
        "    vectorstore = Chroma.from_documents(\n",
        "        documents=splits,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=persist_directory\n",
        "    )\n",
        "    vectorstore.persist()\n",
        "    return vectorstore\n",
        "\n",
        "def detect_anomaly(response: str, question: str) -> str:\n",
        "    \"\"\"Enhanced anomaly detection with financial context.\"\"\"\n",
        "    response_lower = response.lower()\n",
        "    question_lower = question.lower()\n",
        "\n",
        "    # Anomaly indicators\n",
        "    anomaly_indicators = [\n",
        "        'error', 'not found', 'unavailable', 'missing',\n",
        "        'invalid', 'unknown', 'cannot', \"don't know\",\n",
        "        'no data', 'no information', 'failed', 'could not'\n",
        "    ]\n",
        "\n",
        "    # Check for anomaly indicators\n",
        "    if any(indicator in response_lower for indicator in anomaly_indicators):\n",
        "        return \"Anomaly: Missing or invalid data\"\n",
        "\n",
        "    # Financial specific checks\n",
        "    financial_checks = {\n",
        "        'ltv': r'ltv.*?(\\d+\\.?\\d*%)',\n",
        "        'dscr': r'dscr.*?(\\d+\\.?\\d*)',\n",
        "        'cap rate': r'cap(?:italization)? rate.*?(\\d+\\.?\\d*%)',\n",
        "        'noi': r'noi.*?\\$(\\d+\\.?\\d*)',\n",
        "        'ebitda': r'ebitda.*?\\$(\\d+\\.?\\d*)',\n",
        "        'roi': r'roi.*?(\\d+\\.?\\d*%)',\n",
        "        'roa': r'roa.*?(\\d+\\.?\\d*%)'\n",
        "    }\n",
        "\n",
        "    for term, pattern in financial_checks.items():\n",
        "        if term in question_lower:\n",
        "            if not re.search(pattern, response_lower):\n",
        "                return f\"Anomaly: Missing {term.upper()} value\"\n",
        "            else:\n",
        "                # Validate numeric ranges\n",
        "                match = re.search(pattern, response_lower)\n",
        "                value = float(match.group(1).replace('%', ''))\n",
        "\n",
        "                if term == 'ltv' and not (0 <= value <= 100):\n",
        "                    return f\"Anomaly: LTV {value}% out of range (0-100%)\"\n",
        "                elif term == 'dscr' and value < 1.0:\n",
        "                    return f\"Anomaly: DSCR {value} indicates potential cash flow issues\"\n",
        "                elif term == 'cap rate' and not (3 <= value <= 15):\n",
        "                    return f\"Anomaly: Cap rate {value}% outside typical range\"\n",
        "                elif term == 'roi' and not (5 <= value <= 30):\n",
        "                    return f\"Anomaly: ROI {value}% outside typical range\"\n",
        "                elif term == 'roa' and not (5 <= value <= 20):\n",
        "                    return f\"Anomaly: ROA {value}% outside typical range\"\n",
        "\n",
        "    return \"Normal\"\n",
        "\n",
        "def calculate_financial_status(question: str, context: dict) -> dict:\n",
        "    \"\"\"Calculate boolean status for financial questions.\"\"\"\n",
        "    status = {\n",
        "        'question': question,\n",
        "        'status': None,\n",
        "        'reason': None,\n",
        "        'calculated': False\n",
        "    }\n",
        "\n",
        "    question_lower = question.lower()\n",
        "    context_lower = str(context).lower()\n",
        "\n",
        "    # Common financial status checks\n",
        "    if 'healthy' in question_lower or 'stable' in question_lower:\n",
        "        if 'noi' in context_lower or 'net_operating_income' in context_lower:\n",
        "            noi_values = re.findall(r'(?:noi|net operating income).*?\\$([\\d,]+)', context_lower)\n",
        "            if noi_values:\n",
        "                growth = all(float(noi_values[i+1].replace(',', '')) > float(noi_values[i].replace(',', ''))\n",
        "                          for i in range(len(noi_values)-1))\n",
        "                status['status'] = growth\n",
        "                status['reason'] = \"NOI is growing YOY\" if growth else \"NOI growth is not consistent\"\n",
        "                status['calculated'] = True\n",
        "\n",
        "    if 'sufficient' in question_lower and ('dscr' in question_lower or 'coverage' in question_lower):\n",
        "        dscr_values = re.findall(r'dscr.*?([\\d.]+)', context_lower)\n",
        "        if dscr_values:\n",
        "            sufficient = all(float(value) >= 1.25 for value in dscr_values)\n",
        "            status['status'] = sufficient\n",
        "            status['reason'] = \"DSCR meets 1.25 threshold\" if sufficient else \"DSCR below recommended 1.25\"\n",
        "            status['calculated'] = True\n",
        "\n",
        "    if 'reasonable' in question_lower and ('vacancy' in question_lower):\n",
        "        vacancy_values = re.findall(r'vacancy.*?([\\d.]+)%', context_lower)\n",
        "        if vacancy_values:\n",
        "            reasonable = all(float(value) <= 10 for value in vacancy_values)\n",
        "            status['status'] = reasonable\n",
        "            status['reason'] = \"Vacancy rate ≤10%\" if reasonable else \"Vacancy rate >10%\"\n",
        "            status['calculated'] = True\n",
        "\n",
        "    return status\n",
        "\n",
        "# Set up web search tool\n",
        "search = GoogleSearchAPIWrapper(k=1)\n",
        "\n",
        "def enhanced_web_search(query: str) -> str:\n",
        "    \"\"\"Enhanced web search with proper error handling and result formatting.\"\"\"\n",
        "    try:\n",
        "        # The GoogleSearchAPIWrapper.run() only takes the query parameter\n",
        "        results = search.run(query)\n",
        "\n",
        "        if not results:\n",
        "            return \"WEB_SEARCH_UNAVAILABLE\"\n",
        "\n",
        "        # Clean up the results\n",
        "        if isinstance(results, str):\n",
        "            # Return the first non-empty line\n",
        "            for line in results.split('\\n'):\n",
        "                if line.strip():\n",
        "                    return line.strip()\n",
        "            return results.strip()\n",
        "        elif isinstance(results, list):\n",
        "            return results[0] if results else \"WEB_SEARCH_UNAVAILABLE\"\n",
        "        else:\n",
        "            return str(results)\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"WEB_SEARCH_ERROR: {str(e)}\"\n",
        "\n",
        "def custom_agent_executor(question: str, file_paths: List[str] = None) -> str:\n",
        "    \"\"\"Enhanced executor that prioritizes knowledge base over web search.\"\"\"\n",
        "    # Step 1: Try financial data extraction first if documents are uploaded\n",
        "    financial_response = \"\"\n",
        "    if file_paths:\n",
        "        financial_response = answer_financial_question(question, file_paths)\n",
        "        if financial_response != \"Could not find relevant financial data for this question.\":\n",
        "            return financial_response\n",
        "\n",
        "    # Step 2: Try RAG from knowledge base if available\n",
        "    kb_response = None\n",
        "    if vectorstore is not None:\n",
        "        kb_response = answer_with_rag(question)\n",
        "\n",
        "        if kb_response not in [\"KNOWLEDGE_BASE_NOT_AVAILABLE\", \"NO_RELEVANT_DOCUMENTS\", \"ANSWER_NOT_IN_DOCUMENTS\"]:\n",
        "            # Enhance with anomaly detection\n",
        "            anomaly_status = detect_anomaly(kb_response, question)\n",
        "            if \"Anomaly\" in anomaly_status:\n",
        "                kb_response += f\"\\n\\nWARNING: {anomaly_status}\"\n",
        "\n",
        "            # Enhance with financial calculations if relevant\n",
        "            if file_paths:\n",
        "                metrics = extract_financial_metrics(file_paths[0])\n",
        "                calculations = financial_analyzer.calculate_metrics(metrics, question)\n",
        "                if calculations:\n",
        "                    kb_response += \"\\n\\nCalculated Metrics:\\n\" + \"\\n\".join(\n",
        "                        f\"{k}: {v:.2f}{'%' if 'rate' in k.lower() else ''}\"\n",
        "                        for k, v in calculations.items()\n",
        "                    )\n",
        "            return kb_response\n",
        "\n",
        "    # Step 3: Only use web search as last resort for non-financial questions\n",
        "    general_knowledge_keywords = ['capital', 'population', 'president', 'country', 'city']\n",
        "    is_financial_question = any(term in question.lower() for term in [\n",
        "        'rate', 'income', 'expense', 'revenue', 'profit', 'rent', 'loan', 'mortgage', 'dscr', 'noi'\n",
        "    ])\n",
        "\n",
        "    if not is_financial_question or (kb_response in [\"NO_RELEVANT_DOCUMENTS\", \"ANSWER_NOT_IN_DOCUMENTS\"]):\n",
        "        search_response = enhanced_web_search(question)\n",
        "        if search_response not in [\"WEB_SEARCH_UNAVAILABLE\", \"WEB_SEARCH_ERROR\"]:\n",
        "            return f\"Additional information from web search: {search_response}\"\n",
        "\n",
        "    # Final fallback responses\n",
        "    if kb_response == \"KNOWLEDGE_BASE_NOT_AVAILABLE\":\n",
        "        return \"Please upload relevant documents first to create a knowledge base.\"\n",
        "    elif kb_response in [\"NO_RELEVANT_DOCUMENTS\", \"ANSWER_NOT_IN_DOCUMENTS\"]:\n",
        "        return \"The documents don't contain information to answer this question.\"\n",
        "\n",
        "    return \"I couldn't find a definitive answer to your question.\"\n",
        "\n",
        "def answer_with_rag(question: str) -> str:\n",
        "    \"\"\"Improved RAG that better handles financial questions.\"\"\"\n",
        "    if vectorstore is None:\n",
        "        return \"KNOWLEDGE_BASE_NOT_AVAILABLE\"\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Get more context\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return \"NO_RELEVANT_DOCUMENTS\"\n",
        "\n",
        "    # Build context with special handling for tables\n",
        "    context = \"\"\n",
        "    for doc in retrieved_docs:\n",
        "        content = doc.page_content\n",
        "        # Preserve table-like structures\n",
        "        if '\\t' in content or any(term in content.lower() for term in ['year', 'period', 'amount']):\n",
        "            context += f\"\\n\\nTABLE EXCERPT:\\n{content}\\n\"\n",
        "        else:\n",
        "            clean_text = \"\\n\".join([line.strip() for line in content.split('\\n') if line.strip()])\n",
        "            context += f\"\\n\\nTEXT EXCERPT:\\n{clean_text}\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"Based on the financial document excerpts below, answer this question precisely. \"\n",
        "        f\"If the information is in a table, provide exact numbers. \"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Context: {context}\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    parsed_response = ThinkTagOutputParser().parse(response.content)\n",
        "\n",
        "    if not parsed_response or \"I don't know\" in parsed_response.lower():\n",
        "        return \"ANSWER_NOT_IN_DOCUMENTS\"\n",
        "\n",
        "    return parsed_response\n",
        "    \"\"\"Improved RAG that better handles financial questions.\"\"\"\n",
        "    if vectorstore is None:\n",
        "        return \"KNOWLEDGE_BASE_NOT_AVAILABLE\"\n",
        "\n",
        "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})  # Get more context\n",
        "    retrieved_docs = retriever.invoke(question)\n",
        "\n",
        "    if not retrieved_docs:\n",
        "        return \"NO_RELEVANT_DOCUMENTS\"\n",
        "\n",
        "    # Build context with special handling for tables\n",
        "    context = \"\"\n",
        "    for doc in retrieved_docs:\n",
        "        content = doc.page_content\n",
        "        # Preserve table-like structures\n",
        "        if '\\t' in content or any(term in content.lower() for term in ['year', 'period', 'amount']):\n",
        "            context += f\"\\n\\nTABLE EXCERPT:\\n{content}\\n\"\n",
        "        else:\n",
        "            clean_text = \"\\n\".join([line.strip() for line in content.split('\\n') if line.strip()])\n",
        "            context += f\"\\n\\nTEXT EXCERPT:\\n{clean_text}\\n\"\n",
        "\n",
        "    prompt = (\n",
        "        f\"Based on the financial document excerpts below, answer this question precisely. \"\n",
        "        f\"If the information is in a table, provide exact numbers. \"\n",
        "        f\"Question: {question}\\n\"\n",
        "        f\"Context: {context}\\n\"\n",
        "        f\"Answer:\"\n",
        "    )\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    parsed_response = ThinkTagOutputParser().parse(response.content)\n",
        "\n",
        "    if not parsed_response or \"I don't know\" in parsed_response.lower():\n",
        "        return \"ANSWER_NOT_IN_DOCUMENTS\"\n",
        "\n",
        "    return parsed_response\n",
        "\n",
        "\n",
        "def pymupdf_extraction_tool(file_path: str) -> str:\n",
        "    \"\"\"Tool for extracting text, tables, and images from PDF using PyMuPDF.\"\"\"\n",
        "    try:\n",
        "        extracted_content, _ = extract_with_pymupdf(file_path)\n",
        "        return extracted_content if not isinstance(extracted_content, str) or not extracted_content.startswith(\"Error\") else \"Extraction failed\"\n",
        "    except Exception as e:\n",
        "        return f\"Error in PyMuPDF extraction tool: {str(e)}\"\n",
        "\n",
        "# Update the tools list\n",
        "tools = [\n",
        "    Tool(\n",
        "        name=\"Knowledge_Base\",\n",
        "        func=answer_with_rag,\n",
        "        description=\"Useful for answering questions about uploaded documents and financial data.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Financial_Calculator\",\n",
        "        func=lambda q: str(calculate_financial_status(q, {})),  # Context added during execution\n",
        "        description=\"Useful for calculating financial ratios and status indicators.\"\n",
        "    ),\n",
        "   Tool(\n",
        "        name=\"Web_Search\",\n",
        "        func=enhanced_web_search,\n",
        "        description=\"Essential for answering general knowledge questions, current events, or any information not contained in the uploaded documents. Always use this for questions about geography, history, people, or when you need the most up-to-date information.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"Anomaly_Detector\",\n",
        "        func=lambda q, r: detect_anomaly(r, q),\n",
        "        description=\"Useful for validating financial data quality and completeness.\"\n",
        "    ),\n",
        "    Tool(\n",
        "        name=\"PDF_Extractor\",\n",
        "        func=pymupdf_extraction_tool,\n",
        "        description=\"Useful for extracting text, tables, and images from PDF documents using PyMuPDF.\"\n",
        "    )\n",
        "]\n",
        "\n",
        "def create_anomaly_matrix(questions: List[str], responses: List[str]) -> plt.Figure:\n",
        "    \"\"\"Create a matrix visualization of question anomalies.\"\"\"\n",
        "    # Prepare data\n",
        "    data = []\n",
        "    for i, (question, response) in enumerate(zip(questions, responses)):\n",
        "        status = detect_anomaly(response, question)  # Now passing both arguments\n",
        "        data.append({\n",
        "            'Question': f\"Q{i+1}\",\n",
        "            'Status': status.split(\":\")[0] if \":\" in status else status,  # Simplify status for visualization\n",
        "            'Response': response[:100] + '...' if len(response) > 100 else response\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Create the visualization\n",
        "    fig, ax = plt.subplots(figsize=(12, max(6, len(questions) * 0.5)))\n",
        "\n",
        "    # Create a color-coded matrix\n",
        "    status_categories = ['Normal', 'Anomaly']\n",
        "    pivot = pd.crosstab(df['Question'], df['Status']).reindex(columns=status_categories, fill_value=0)\n",
        "\n",
        "    # Plot the matrix\n",
        "    sns.heatmap(\n",
        "        pivot,\n",
        "        annot=True,\n",
        "        fmt='d',\n",
        "        cmap=['#51cf66', '#ff6b6b'],  # Green for normal, red for anomaly\n",
        "        cbar=False,\n",
        "        ax=ax\n",
        "    )\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title('Question Response Anomaly Detection')\n",
        "    ax.set_ylabel('Questions')\n",
        "    ax.set_xlabel('Status')\n",
        "    plt.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def process_input(questions: str, files: List[str]) -> tuple:\n",
        "    \"\"\"Process user input and return response and anomaly matrix.\"\"\"\n",
        "    global vectorstore\n",
        "    file_paths = [file.name for file in files] if files else None\n",
        "\n",
        "    if files and (vectorstore is None or len(files) > 0):\n",
        "        vectorstore = create_vector_db_from_files(file_paths)\n",
        "\n",
        "    questions_list = [q.strip() for q in questions.split(\"\\n\") if q.strip()]\n",
        "    responses = []\n",
        "\n",
        "    for question in questions_list:\n",
        "        try:\n",
        "            response = custom_agent_executor(question, file_paths)\n",
        "            responses.append(response)\n",
        "        except Exception as e:\n",
        "            responses.append(f\"Error processing question: {str(e)}\")\n",
        "\n",
        "    # Create formatted response text\n",
        "    formatted_responses = [\n",
        "        f\"**Q:** {q}\\n**A:** {r}\\n\"\n",
        "        for q, r in zip(questions_list, responses)\n",
        "    ]\n",
        "    full_response = \"\\n\".join(formatted_responses)\n",
        "\n",
        "    # Create anomaly matrix\n",
        "    chart = create_anomaly_matrix(questions_list, responses)\n",
        "\n",
        "    return full_response, chart\n",
        "\n",
        "# Gradio UI\n",
        "with gr.Blocks(title=\"Financial Audit Assistant\") as demo:\n",
        "    gr.Markdown(\"# Financial Audit Assistant\")\n",
        "    gr.Markdown(\"Upload documents and ask questions about loan terms and financial metrics.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            file_input = gr.File(\n",
        "                label=\"Upload PDF Documents\",\n",
        "                file_count=\"multiple\",\n",
        "                file_types=[\".pdf\"],\n",
        "                type=\"filepath\"\n",
        "            )\n",
        "            question_input = gr.Textbox(\n",
        "                label=\"Questions (one per line)\",\n",
        "                lines=10,\n",
        "                placeholder=\"Enter your questions here, one per line...\\nExample:\\nWhat's the loan type?\\nWhat's the LTV for this loan?\\nWhat is the DSCR?\"\n",
        "            )\n",
        "            submit_btn = gr.Button(\"Submit\")\n",
        "\n",
        "        with gr.Column():\n",
        "            output_response = gr.Markdown(label=\"Response\")\n",
        "            chart_output = gr.Plot(label=\"Financial Metrics\")\n",
        "\n",
        "    submit_btn.click(\n",
        "        fn=process_input,\n",
        "        inputs=[question_input, file_input],\n",
        "        outputs=[output_response, chart_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #generate_architecture_diagram() # Generate the diagram before launching Gradio\n",
        "  demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "f5o81mg92UJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwtc6d4amoEGe6rnt+Gc+j",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}